# Project name:

**Perception of vowel features (formant structure, pitch) in children with autism spectrum disorders and typically developing children (MEG/ERF study).**

### Related publications:
- **Peer-reviewed publication**: Fadeev, K. A., Romero Reyes, I. V., Goiaeva, D. E., Obukhova, T. S., Ovsiannikova, T. M., Prokofyev, A. O., Rytikova, A. M., Novikov, A. Y., Kozunov, V. V., Stroganova, T. A., & Orekhova, E. V. (2024). Attenuated processing of vowels in the left temporal cortex predicts speech-in-noise perception deficit in children with autism. Journal of Neurodevelopmental Disorders, 16(1), 1–24. [https://doi.org/10.1186/s11689-024-09585-2](https://doi.org/10.1186/s11689-024-09585-2) 
- **Preprint in bioRxiv**: Fadeev, K. A., Romero Reyes, I. V., Goiaeva, D. E., Obukhova, T. S., Ovsiannikova, T. M., Prokofyev, A. O., Rytikova, A. M., Novikov, A. Y., Kozunov, V. V., Stroganova, T. A., & Orekhova, E. V. (2024). Attenuated Processing of Vowels in the Left Hemisphere Predicts Speech-in-Noise Perception Deficit in Children with Autism. [https://doi.org/10.1101/2024.06.24.600191](https://doi.org/10.1101/2024.06.24.600191)

### Year(s) that the project ran:

2017-2024.

# Brief overview:

This dataset was obtained by the team at the Center for Neurocognitive Research (MEG Center) of Moscow State University of Psychology and Education as part of a study on vowel perception and their properties in children (Fadeev et al., 2024, preprint). It includes MEG recordings from 35 children with autism spectrum disorders and 39 typically developing children.

# Experimental procedure:

The participants were instructed to watch a silent video (movie/cartoon) of their choice and to ignore the auditory stimuli. The stimuli were delivered binaurally via plastic ear tubes inserted into the ear canals. The tubes were fixed to the MEG helmet to minimize possible noise from contact with the subject’s clothing. The intensity was set at a sound pressure level of 90 dB SPL. The experiment consisted of three blocks of 360 trials, each block lasting around 9 minutes with short breaks between blocks.

# Stimuli:

The experimental paradigm used in this study is identical to that described in Orekhova et al. (2023). We used four types of synthetic vowel-like stimuli previously employed by Uppenkamp et al. (2006, 2011) and downloaded from [http://medi.uni-oldenburg.de/members/stefan/phonology_1/](http://medi.uni-oldenburg.de/members/stefan/phonology_1/). **You can also find a copy of the sound files in the `stimuli` directory on this dataset page**. Five strong vowels were used: 
- /a/ (caw, dawn), 
- /e/ (ate, bait), 
- /i/ (beat, peel), 
- /o/ (coat, wrote) and 
- /u/ (boot, pool). 

A total of 270 stimuli from each of the four classes were presented, with three stimulus variants equally represented within each class (N = 90). All stimuli were presented in random order. Each stimulus lasted 812 ms, including rise/fall times of 10 ms each. The interstimulus intervals (ISI) were randomly chosen from a range of 500 to 800 ms.

Short names of stimuli (used in code, filenames, and directory names):
- `dv` - periodic vowels
- `rv` - non-periodic vowels
- `mp` - periodic non-vowels
- `mr` - non-periodic non-vowels

# Additional data acquired:

The following tests were also administered to the study participants: 
- Pure tone air conduction audiometry;
- Russian Child Language Assessment Battery (Lopukhina et al., 2019);
- Words-in-Noise (WiN) test (Fadeev et al., 2023);
- Social Responsiveness Scale for children (Constantino, 2013);
- Social Communication Questionnaire (SCQ-Lifetime) (Berument et al., 1999),
- KABC-II, Mental Processing Index (MPI) as an IQ equivalent (Kaufman & Kaufman, 2004).

# Dataset content:

## MEG data
`sub-LABEL/meg/...meg.fif` -- 3 runs (in some cases, the number of runs may differ due to the subjects' features). MEG data were recorded using Elekta VectorView Neuromag 306-channel MEG detector array (Helsinki, Finland) with 0.1 - 330 Hz inbuilt filters and 1000 Hz sample frequency.

## MRI data
`sub-LABEL/anat/` -- T1-weighted images MRI for subject.

## Freesurfer
 `derivatives/freesurfer/` - outputs of running the FreeSurfer pipeline `recon-all` on the MRI data with no additional command line options (only defaults were used):
```
$ recon-all -i sub-Z201_T1w.nii.gz -s Z201 -all
```
After the `recon-all` call, there were further FreeSurfer calls from the MNE
API:
```
$ mne make_scalp_surfaces -s Z201 --force
$ mne watershed_bem -s Z201
```

## Code
The code of the project was developed and tested for Ubuntu OS (20-22 x64 version) and has the following structure (directory names provide explanations of their contents):

```
code
├── analysis
│         ├── 0-preprocessing_for_clustering
│         │         ├── ...
│         ├── 1-tfce_clustering
│         │         ├── ...
│         ├── 2-clustering_results_analytics
│         │         ├── ...
│         └── modules
│             ├── clustering.py
│             └── data_ops.py
├── envs
│         ├── envs_for_between_groups_clustering_in_auditory_cortex_with_morphological_sign_flip.json
│         ├── envs_for_interaction_clustering_in_auditory_cortex_with_morphological_sign_flip.json
│         └── envs_for_within_groups_clustering_in_auditory_cortex_with_morphological_sign_flip.json
├── preprocessing
│         ├── 00-maxwell_filtering.py
│         ├── ...
│         └── 10-make_stc.py
├── README
├── requirements_for_ubuntu_2x.txt
└── requirements_for_windows_1x.txt
```

<font color="#ff0000">**Please read `code/README.md` file for more detail instructions.** </font>

### Requirements for Code Usage (MNE-Python & Additional Python Libraries)"

1. For installation, we recommend the Anaconda distribution. Find the installation guide here: [Anaconda Installation Guide](https://docs.anaconda.com/anaconda/install/).

2. After you have a working version of Python 3, simply install a new virtual environment via this command in the Ubuntu terminal or Anaconda Prompt for Windows OS:

```sh
conda create --name=mne1.4 --channel=conda-forge python=3.10 mne=1.4.1 numpy=1.23.1 spyder pyvista pyvistaqt pingouin scipy mne-bids pandas openpyxl autoreject
```


Or use the requirements_xxxxxx.txt files located in `code` directory:
- For Ubuntu OS (version 20 and above), please use `requirements_for_ubuntu_2x.txt`.
- For Windows OS (version 10 and above), please use `requirements_for_windows_1x.txt`.

To do this, you can run the following command in the Ubuntu terminal or Anaconda Prompt for Windows OS:

```
$ conda create --name mne1.4 --file requirements_filename
```

3. Activate the created environment:
```
$ conda activate mne1.4
```

4. Start your favorite IDE with this environment. For example, to start Spyder IDE, use this command after activating the environment:
```
$ spyder
```

5. To start processing data, go to the directory with the downloaded data and open the Python scripts of interest. (If you are using Spyder IDE, use the "Files" pane located in the upper right corner of the workspace.)

## Clustering results directories

All output files from the scripts of the `code` directory are saved in the `derivatives` directory.

Clustering Versions mnemonics (phrases used in python scripts names and directory names):
- `v13`: TFCE clustering based on groups ⨯ conditions difference response
- `v14`: TFCE clustering based on between-conditions difference response
- `v15`: TFCE clustering based on between groups evoked response

The derivatives of project has the following structure (directory names provide explanations of their contents):

```
derivatives/preprocessing/
├── fsaverage_labels_of_analytics
│         ├── auditory_cortex_region-lh.label
│         └── auditory_cortex_region-rh.label
└── fsaverage_stcs_after_morph_flip_in_labels_of_analytics
    ├── subjects_info_for_morphological_sign_flipped_data_1000Hz
    └── subjects_stc_for_morphological_sign_flipped_data_1000Hz

derivatives/analysis/
├── 20240607_74subj_v13_500Hz_5000_perm_DV-MR_TD_vs_ASD_0-800_msec_tfce_interaction_in_auditory_cortex_morph_flip_with_5e-02_clusters_p_thresh
├── 20240608_74subj_v13_500Hz_5000_perm_MP-MR_TD_vs_ASD_0-800_msec_tfce_interaction_in_auditory_cortex_morph_flip_with_5e-02_clusters_p_thresh
├── 20240608_74subj_v13_500Hz_5000_perm_RV-MR_TD_vs_ASD_0-800_msec_tfce_interaction_in_auditory_cortex_morph_flip_with_5e-02_clusters_p_thresh
├── 20240608_74subj_v14_500Hz_5000perm_ASD_DV-MR_0-800_msec_tfce_1samp_within_groups_in_auditory_cortex_morph_flip_5e-02_clusters_p_thresh
├── 20240608_74subj_v14_500Hz_5000perm_ASD_MP-MR_0-800_msec_tfce_1samp_within_groups_in_auditory_cortex_morph_flip_5e-02_clusters_p_thresh
├── 20240608_74subj_v14_500Hz_5000perm_ASD_RV-MR_0-800_msec_tfce_1samp_within_groups_in_auditory_cortex_morph_flip_5e-02_clusters_p_thresh
├── 20240608_74subj_v14_500Hz_5000perm_TD_DV-MR_0-800_msec_tfce_1samp_within_groups_in_auditory_cortex_morph_flip_5e-02_clusters_p_thresh
├── 20240608_74subj_v14_500Hz_5000perm_TD_MP-MR_0-800_msec_tfce_1samp_within_groups_in_auditory_cortex_morph_flip_5e-02_clusters_p_thresh
├── 20240608_74subj_v14_500Hz_5000perm_TD_RV-MR_0-800_msec_tfce_1samp_within_groups_in_auditory_cortex_morph_flip_5e-02_clusters_p_thresh
├── 20240608_74subj_v15_500Hz_5000_perm_DV_TD_vs_ASD_in_0-800_msec_tfce_between_groups_in_auditory_cortex_morph_flip_with_5e-02_cluster_p_threshold
├── 20240608_74subj_v15_500Hz_5000_perm_MP_TD_vs_ASD_in_0-800_msec_tfce_between_groups_in_auditory_cortex_morph_flip_with_5e-02_cluster_p_threshold
└── 20240608_74subj_v15_500Hz_5000_perm_RV_TD_vs_ASD_in_0-800_msec_tfce_between_groups_in_auditory_cortex_morph_flip_with_5e-02_cluster_p_threshold

```

# Data user agreement:

Dataset is distributed under the CC BY license. According to this license, when using this dataset, including a reference link to it (i.e., by DOI) is mandatory.

# Acknowledgements:

We sincerely thank all of volunteers who participated in this study.

# Funding:

The study was funded within the framework of the state assignment of the Ministry of Education of the Russian Federation (N 073-00037-24-01).

# References:

1. Gutschalk, A., & Uppenkamp, S. (2011). Sustained responses for pitch and vowels map to similar sites in human auditory cortex. Neuroimage, 56(3), 1578-1587. doi:10.1016/j.neuroimage.2011.02.026

2.  Orekhova, E. V., Fadeev, K. A., Goiaeva, D. E., Obukhova, T. S., Ovsiannikova, T. M., Prokofyev, A. O., & Stroganova, T. A. (2023). Different Hemispheric Lateralization for Periodicity and Formant Structure of Vowels in the Auditory Cortex and Its Changes between Childhood and Adulthood. Cortex. doi:10.1016/j.cortex.2023.10.020

3. Uppenkamp, S., Johnsrude, I. S., Norris, D., Marslen-Wilson, W., & Patterson, R. D. (2006). Locating the initial stages of speech-sound processing in human temporal cortex. Neuroimage, 31(3), 1284-1296. doi:10.1016/j.neuroimage.2006.01.004

4. Fadeev, K. A., Goyaeva, D. E., Obukhova, T. S., Ovsyannikova, T. M., Shvedovskiy, E. F., Yu. Nikolaeva, A., Davydova, E. Y., Stroganova, T. A. & Orekhova, E. V. (2023). Difficulty with Speech Perception in the Background of Noise in Children with Autism Spectrum Disorders Is Not Related to Their Level of Intelligence. Clinical Psychology and Special Education, 12(1), 180–212. https://doi.org/10.17759/cpse.2023120108

5. Fadeev, K. A., Romero Reyes, I. V., Goiaeva, D. E., Obukhova, T. S., Ovsiannikova, T. M., Prokofyev, A. O., Rytikova, A. M., Novikov, A. Y., Kozunov, V. V., Stroganova, T. A., & Orekhova, E. V. (2024). Attenuated processing of vowels in the left hemisphere predicts speech-in-noise perception deficit in children with autism. bioRxiv (Cold Spring Harbor Laboratory). https://doi.org/10.1101/2024.06.24.600191

6. Lopukhina, A., Chrabaszcz, A., Khudyakova, M., Korkina, I., Yurchenko, A. & Dragoy, O. (2019). Test for assessment of language development in Russian «KORABLIK». Proceedings of the Satellite of AMLaP conference «Typical and Atypical Language Development Symposium».

7. Kaufman, A. S. & Kaufman, N. L. (2004). KABC-II : Kaufman Assessment Battery for Children (2nd ed., Vol. 1–8). AGS Pub.

8. Berument, S. K., Rutter, M., Lord, C., Pickles, A. & Bailey, A. (1999). Autism screening questionnaire: diagnostic validity. The British Journal of Psychiatry: The Journal of Mental Science, 175, 444–451. https://doi.org/10.1192/bjp.175.5.444

9. Constantino, J. N. (2013). Social Responsiveness Scale. In F. R. Volkmar (Ed.), Encyclopedia of Autism Spectrum Disorders (pp. 2919–2929). Springer New York. https://doi.org/10.1007/978-1-4419-1698-3_296
